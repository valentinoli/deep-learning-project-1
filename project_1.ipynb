{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helpers import grid_search, tune_hyperparameters\n",
    "\n",
    "##tmp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "epochs = 25\n",
    "mini_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################    \n",
    "# generate kfold cross validation pairs\n",
    "# keep this pair constant throughout the tuning process\n",
    "######################################################    \n",
    "\n",
    "kfold_input, kfold_target, kfold_classes, \\\n",
    "    test_input, test_target, test_classes = \\\n",
    "    prologue.generate_pair_sets(N)\n",
    "\n",
    "k_folds = 4\n",
    "shuffle_indices = torch.randperm(len(kfold_input))\n",
    "split_indices = torch.split(shuffle_indices, int(torch.tensor(len(kfold_input) / k_folds).item()))\n",
    "\n",
    "\n",
    "kfold_train_dict = {}\n",
    "kfold_validation_dict = {}\n",
    "\n",
    "for i in range(k_folds):\n",
    "    kfold_train_indices = torch.cat(split_indices[0:i] + split_indices[i+1:])\n",
    "    kfold_validation_indices = split_indices[i]\n",
    "\n",
    "    \n",
    "    kfold_train_dict[i] = {'input':kfold_input[kfold_train_indices],'target':kfold_target[kfold_train_indices],'classes':kfold_classes[kfold_train_indices]}\n",
    "    kfold_validation_dict[i] = {'input':kfold_input[kfold_validation_indices],'target':kfold_target[kfold_validation_indices],'classes':kfold_classes[kfold_validation_indices]}\n",
    "\n",
    "    \n",
    "######################################################    \n",
    "# kfold cross validation for lambda and learn rate   #\n",
    "######################################################    \n",
    "lambdas = torch.logspace(start=-2, end=0, steps=5)\n",
    "learn_rates = torch.logspace(start=-4, end=1, steps=10)\n",
    "learn_rates2 = torch.logspace(start=-4, end=1, steps=5)\n",
    "\n",
    "model_noaux = ['siam','naive']\n",
    "model_aux = ['siam_aux_hidden','siam_aux']\n",
    "\n",
    "grid_model_dict = {}\n",
    "\n",
    "print('======================================================')\n",
    "print('Tuning lambda and learn rate for each architecture....')\n",
    "\n",
    "for m in model_noaux:\n",
    "    train_error = []\n",
    "    validation_error = []\n",
    "    losses_train = []\n",
    "    \n",
    "    for lr in learn_rates:\n",
    "        if m == 'naive':\n",
    "            Model = NaiveCNN(1)\n",
    "        elif m == 'siam':\n",
    "            Model = SharedWeight(1)\n",
    "        else:\n",
    "            print('invalid model type')\n",
    "            break\n",
    "\n",
    "        kfold_losses_train = []\n",
    "        kfold_train_error = []\n",
    "        kfold_validation_error = []\n",
    "\n",
    "        for i in range(k_folds):\n",
    "            kfold_losses_train.append(train_model(Model, kfold_train_dict[i]['input'], kfold_train_dict[i]['target'],\\\n",
    "                                        learn_rate_= lr, lambda_=0, mini_batch_size=mini_batch_size, nb_epochs = 25))\n",
    "\n",
    "            kfold_train_error.append(compute_nb_errors(Model, kfold_train_dict[i]['input'], kfold_train_dict[i]['target'], mini_batch_size))\n",
    "            kfold_validation_error.append(compute_nb_errors(Model, kfold_validation_dict[i]['input'], kfold_validation_dict[i]['target'], mini_batch_size))\n",
    "\n",
    "        train_error.append(torch.tensor(kfold_train_error ,dtype=torch.float).mean())\n",
    "        losses_train.append(torch.tensor(kfold_losses_train ,dtype=torch.float).mean())\n",
    "        validation_error.append(torch.tensor(kfold_validation_error ,dtype=torch.float).mean())\n",
    "        \n",
    "    grid_model_dict[m] = {'train_error':train_error, 'validation_error':validation_error, 'losses_train':losses_train}\n",
    "#    print(m)\n",
    "    \n",
    "for m in model_aux:\n",
    "    train_error = []\n",
    "    validation_error = []\n",
    "    losses_train = []\n",
    "\n",
    "    for lbd in lambdas:\n",
    "    # optimize the learning rate\n",
    "        for lr in learn_rates2:\n",
    "            if m == 'siam_aux':\n",
    "                Model = SharedWeight(1)\n",
    "            elif m == 'siam_aux_hidden':\n",
    "                Model = SharedWeight(2)\n",
    "            else:\n",
    "                print('invalid model type')\n",
    "                break\n",
    "            \n",
    "            kfold_losses_train = []\n",
    "            kfold_train_error = []\n",
    "            kfold_validation_error = []\n",
    "\n",
    "            for i in range(k_folds):\n",
    "\n",
    "                \n",
    "                kfold_losses_train.append(train_model(Model, kfold_train_dict[i]['input'], kfold_train_dict[i]['target'], kfold_train_dict[i]['classes'], \\\n",
    "                            learn_rate_= lr, lambda_=lbd, mini_batch_size=mini_batch_size, nb_epochs = 25))\n",
    "\n",
    "                kfold_train_error.append(compute_nb_errors(Model, kfold_train_dict[i]['input'], kfold_train_dict[i]['target'], mini_batch_size))\n",
    "                kfold_validation_error.append(compute_nb_errors(Model, kfold_validation_dict[i]['input'], kfold_validation_dict[i]['target'], mini_batch_size))\n",
    "\n",
    "            train_error.append(torch.tensor(kfold_train_error ,dtype=torch.float).mean())\n",
    "            losses_train.append(torch.tensor(kfold_losses_train ,dtype=torch.float).mean())\n",
    "            validation_error.append(torch.tensor(kfold_validation_error ,dtype=torch.float).mean())\n",
    "                        \n",
    "    grid_model_dict[m] = {'train_error':train_error, 'validation_error':validation_error, 'losses_train':losses_train}\n",
    "#    print(m)\n",
    "\n",
    "\n",
    "######################################################    \n",
    "# getting lambda and learn rate for each model   #\n",
    "# based on validation error\n",
    "######################################################    \n",
    "\n",
    "hyperparam_pair = []\n",
    "for lbd in lambdas:\n",
    "    for lr in learn_rates2:\n",
    "        hyperparam_pair.append({'lambda': lbd,'learn_rate': lr})\n",
    "\n",
    "hyperparam_dict_1 = {}\n",
    "for m in model_noaux:\n",
    "    hyperparam_dict_1[m] = {'lambda':0, 'learn_rate':learn_rates[torch.argmin(torch.tensor(grid_model_dict[m]['validation_error']))]}\n",
    "\n",
    "for m in model_aux: \n",
    "    p = hyperparam_pair[torch.argmin(torch.tensor(grid_model_dict[m]['validation_error']))]\n",
    "    hyperparam_dict_1[m] = {'lambda':p['lambda'], 'learn_rate':p['learn_rate']}\n",
    "\n",
    "\n",
    "print('Finished tuning lambda and learn rate for each architecture')\n",
    "print(hyperparam_dict_1)\n",
    "\n",
    "\n",
    "######################################################    \n",
    "# kfold cross validation for batch size   #\n",
    "######################################################    \n",
    "\n",
    "print('======================================================')\n",
    "print('Tuning batch size for each architecture....')\n",
    "\n",
    "batchsizes = [5,10,25,50,125]\n",
    "batchsize_dict = {}\n",
    "\n",
    "model_type = ['naive','siam','siam_aux','siam_aux_hidden']\n",
    "for m in model_type:\n",
    "    losses_train = []\n",
    "    train_error = []\n",
    "    validation_error = []\n",
    "    lbd = hyperparam_dict_1[m]['lambda']\n",
    "    lr = hyperparam_dict_1[m]['learn_rate']\n",
    "\n",
    "\n",
    "    for batch in batchsizes:\n",
    "        if m == 'naive':\n",
    "            Model = NaiveCNN(1)\n",
    "        elif m == 'siam' or 'siam_aux':\n",
    "            Model = SharedWeight(1)\n",
    "        elif m == 'siam_aux_hidden':\n",
    "            Model = SharedWeight(2)\n",
    "        else:\n",
    "            print('invalid model type')\n",
    "            break\n",
    "\n",
    "        kfold_losses_train = []\n",
    "        kfold_train_error = []\n",
    "        kfold_validation_error = []\n",
    "\n",
    "        \n",
    "        for i in range(k_folds):\n",
    "            if m == 'siam_aux' or 'siam_aux_hidden':\n",
    "                kfold_losses_train.append(train_model(Model, kfold_train_dict[i]['input'], kfold_train_dict[i]['target'], kfold_train_dict[i]['classes'], \\\n",
    "                            learn_rate_= lr, lambda_=lbd, mini_batch_size=mini_batch_size, nb_epochs = 25))\n",
    "            else:\n",
    "                kfold_losses_train.append(train_model(Model, kfold_train_dict[i]['input'], kfold_train_dict[i]['target'],\\\n",
    "                                            learn_rate_= lr, lambda_=lbd, mini_batch_size=batch, nb_epochs = 25))\n",
    "                \n",
    "            kfold_train_error.append(compute_nb_errors(Model, kfold_train_dict[i]['input'], kfold_train_dict[i]['target'], batch))\n",
    "            kfold_validation_error.append(compute_nb_errors(Model, kfold_validation_dict[i]['input'], kfold_validation_dict[i]['target'], batch))\n",
    "\n",
    "        train_error.append(torch.tensor(kfold_train_error ,dtype=torch.float).mean())\n",
    "        losses_train.append(torch.tensor(kfold_losses_train ,dtype=torch.float).mean())\n",
    "        validation_error.append(torch.tensor(kfold_validation_error ,dtype=torch.float).mean())\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.semilogx(batchsizes, train_error,c='steelblue',label=\"train_error\", marker='x')\n",
    "    plt.semilogx(batchsizes, validation_error,c='orangered', label=\"validation_error\", marker='x')\n",
    "    plt.title(m + \" network hyperparameter optimization - mini batch size\")\n",
    "    plt.xlabel(\"mini batch size\")\n",
    "    plt.ylabel(\"nb error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    fig.savefig(m + \"_batch_size_CV.pdf\") # Use fig. here\n",
    "\n",
    "    batchsize_dict[m] = batchsizes[torch.argmin(torch.tensor(validation_error))]\n",
    "    \n",
    "######################################################    \n",
    "# create a complex of hyperparam   #\n",
    "######################################################      \n",
    "hyperparam_dict = {}\n",
    "for m in model_type:\n",
    "    hyperparam_dict[m] = {'lambda':hyperparam_dict_1[m]['lambda'], 'learn_rate':hyperparam_dict_1[m]['learn_rate'], 'batchsize':batchsize_dict[m]}\n",
    "\n",
    "print('Finished tuning batch size for each architecture')\n",
    "print('This is the final aggregation of the hyperparams')    \n",
    "print(hyperparam_dict)    \n",
    "\n",
    "\n",
    "######################################################    \n",
    "# test the models and compare the performance  #\n",
    "######################################################      \n",
    "\n",
    "print('======================================================')\n",
    "print('Testing the model performances....')\n",
    "\n",
    "import numpy as np\n",
    "#generate boxplot across models\n",
    "SIAM_auxi_train_error = []\n",
    "SIAM_auxi_test_error = []\n",
    "SIAM_train_error = []\n",
    "SIAM_test_error = []\n",
    "NAIVE_train_error = []\n",
    "NAIVE_test_error = []\n",
    "SIAM_auxi_5_train_error = []\n",
    "SIAM_auxi_5_test_error = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    train_input, train_target, train_classes, \\\n",
    "    test_input, test_target, test_classes = \\\n",
    "    prologue.generate_pair_sets(N)\n",
    "    \n",
    "    NaiveCN = NaiveCNN(1)\n",
    "    Siamese = SharedWeight(1)\n",
    "    Siamese_auxi = SharedWeight(1)\n",
    "    Siamese_auxi_5 = SharedWeight(2)\n",
    "    \n",
    "    hnaive = hyperparam_dict['naive']\n",
    "    hsiam = hyperparam_dict['siam']\n",
    "    hsiamaux = hyperparam_dict['siam_aux']\n",
    "    hsiamhidaux= hyperparam_dict['siam_aux_hidden']\n",
    "    \n",
    "    train_model(NaiveCN, train_input, train_target, learn_rate_= hnaive['learn_rate'], lambda_=hnaive['lambda'], mini_batch_size=hnaive['batchsize'], nb_epochs = 25)\n",
    "    train_model(Siamese, train_input, train_target, learn_rate_= hsiam['learn_rate'], lambda_=hsiam['lambda'], mini_batch_size=hsiam['batchsize'], nb_epochs = 25)\n",
    "    train_model(Siamese_auxi, train_input, train_target, train_classes, learn_rate_=hsiamaux['learn_rate'], lambda_=hsiamaux['lambda'], mini_batch_size=hsiamaux['batchsize'], nb_epochs = 25)\n",
    "    train_model(Siamese_auxi_5, train_input, train_target, train_classes, learn_rate_= hsiamhidaux['learn_rate'], lambda_=hsiamhidaux['lambda'], mini_batch_size=hsiamhidaux['batchsize'], nb_epochs = 25)\n",
    "    \n",
    "    SIAM_auxi_5_train_error.append(compute_nb_errors(Siamese_auxi_5, train_input, train_target, hsiamhidaux['batchsize']))\n",
    "    SIAM_auxi_5_test_error.append(compute_nb_errors(Siamese_auxi_5, test_input, test_target, hsiamhidaux['batchsize']))\n",
    "    SIAM_auxi_train_error.append(compute_nb_errors(Siamese_auxi, train_input, train_target, hsiamaux['batchsize']))\n",
    "    SIAM_auxi_test_error.append(compute_nb_errors(Siamese_auxi, test_input, test_target, hsiamaux['batchsize']))\n",
    "    SIAM_train_error.append(compute_nb_errors(Siamese, train_input, train_target, hsiam['batchsize']))\n",
    "    SIAM_test_error.append(compute_nb_errors(Siamese, test_input, test_target, hsiam['batchsize']))\n",
    "    NAIVE_train_error.append(compute_nb_errors(NaiveCN, train_input, train_target, hnaive['batchsize']))\n",
    "    NAIVE_test_error.append(compute_nb_errors(NaiveCN, test_input, test_target, hnaive['batchsize']))\n",
    "    \n",
    "fig = plt.figure(figsize=(12, 6), dpi=80)\n",
    "vals =[np.array(NAIVE_train_error)/10,\n",
    "       np.array(NAIVE_test_error)/10,\n",
    "       np.array(SIAM_train_error)/10,\n",
    "       np.array(SIAM_test_error)/10,\n",
    "       np.array(SIAM_auxi_train_error)/10,\n",
    "       np.array(SIAM_auxi_test_error)/10,\n",
    "       np.array(SIAM_auxi_5_train_error)/10,\n",
    "       np.array(SIAM_auxi_5_test_error)/10\n",
    "      ]\n",
    "labels= [\"NAIVE train\\n(1 hidden)\",\n",
    "         \"NAIVE test\\n(1 hidden)\",\n",
    "         \"SIAM_train\\n(1 hidden)\",\n",
    "         \"SIAM_test\\n(1 hidden)\",\n",
    "         \"SIAM+auxi train\\n(1 hidden)\",\n",
    "         \"SIAM+auxi test\\n(1 hidden)\",\n",
    "         \"SIAM+auxi train\\n(2 hidden)\",\n",
    "         \"SIAM+auxi test\\n(2 hidden)\"\n",
    "        ]\n",
    "\n",
    "plt.boxplot(vals, labels=labels, zorder=1)\n",
    "for i in range(len(vals)):\n",
    "    plt.plot([i+1]*10,vals[i],\".\", markersize=7, label=labels[i].replace('\\n', ' '))\n",
    "plt.hlines(15, xmin=0.5, xmax=8.5, ls='--', color='gray')\n",
    "plt.title(\"Models performance comparison\")\n",
    "plt.ylabel(\"error [%]\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(\"models_error_boxplot_2.pdf\")    \n",
    "\n",
    "print('Finished testing the model performances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in Siamese_auxi_5.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in Siamese_auxi.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in Siamese.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [\n",
    "        NAIVE_test_error,\n",
    "        SIAM_test_error,\n",
    "        SIAM_auxi_test_error,\n",
    "        SIAM_auxi_5_test_error\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val)):\n",
    "    print((np.random.choice(val[i], 1000)/10).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val)):\n",
    "    print((np.random.choice(val[i], 1000)/10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(val)):\n",
    "    print(100-(np.random.choice(val[i], 1000)/10).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-env",
   "language": "python",
   "name": "deeplearning-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
